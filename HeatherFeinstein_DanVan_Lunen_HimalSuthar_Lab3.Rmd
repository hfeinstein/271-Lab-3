---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271):
  Lab 3'
author: "Heather Feinstein, Himal Suthar, Daniel Vanlunen"
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

# Instructions:

*  $\textbf{Due Date: Monday of Week 11 4p.m. Pacific Time}$
*  $\textbf{Page limit of the pdf report for Question 1: 12 (not include title and the table of content page. No page limit for Question 2.}$
  * Use the margin, linespace, and font size specification below:
    * fontsize=11pt
    * margin=1in
    * line_spacing=single

* Submission:
    * Each group makes one submission to Github; please have one of your team members made the submission
    * Submit 2 files:
        1. A pdf file including the details of your analysis and all the R codes used to produce the analysis. Please do not suppress the codes in your pdf file.
        2. R markdown file used to produce the pdf file
    * Use the following file-naming convensation; fail to do so will receive 10% reduction in the grade:
        * FirstNameLastName1_FirstNameLastName2_FirstNameLastName3_LabNumber.fileExtension
        * For example, if you have three students in the group for Lab Z, and their names are Gerard Kelley, Steve Yang, and Jeffrey Yau, then you should name your file the following
            * GerardKelley_SteveYang_JeffreyYau_LabZ.Rmd
            * GerardKelley_SteveYang_JeffreyYau_LabZ.pdf
    * Although it sounds obvious, please write the name of each members of your group on page 1 of your pdf and Rmd files.
    * This lab can be completed in a group of up to 3 students in your session. Students are encouraged to work in a group for the lab.

* Other general guidelines:
    * For statistical methods that we cover in this course, use only the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you have to provide (1) explanation of why such libraries and functions are used instead and (2) reference to the library documentation. Lacking the explanation and reference to the documentation will result in a score of zero for the corresponding question.

  * Students are expected to act with regards to UC Berkeley Academic Integrity.


# Question 1: Forecasting using a SARIMA model

*ECOMPCTNSA.csv*, contains quarterly data of E-Commerce Retail Sales as a Percent of Total Sales. The data can be found at: https://fred.stlouisfed.org/series/ECOMPCTNSA.

Build a Seasonal ARIMA model and generate quarterly forecast for 2017. Make sure you use all the steps of building a univariate time series model between lecture 6 and 9, such as checking the raw data, conducting a thorough EDA, justifying all modeling decisions (including transformation), testing model assumptions, and clearly articulating why you chose your given model. Measure and discuss your model's performance. Use both in-sample and out-of-sample model performance. When estimating your model, exclude the series from 2015 and 2016. For the out-of-sample forecast, measure your model's performance in forecasting the quarterly E-Commerce retail sales in 2015 and 2016. Discuss the model performance. Also forecast beyond the observed time-period of the series. Specifically, generate quarterly forecast for 2017.

```{r include=F}
# Load required libraries
library(tidyverse)
library(car)
library(Hmisc)
library(ggfortify)
library(plotly)
library(astsa)
library(forecast)
library(fpp2)
library(gridExtra)
library(urca)
library(glue)
```
## EDA
First we load and examine the raw data.

```{r}
df <- read_csv("ECOMPCTNSA.csv")

# Examine the data structure
head(df)
summary(df)
describe(df)

# Convert it into a time seriea object
series1 <- ts(df$ECOMPCTNSA/100, start=c(1999,4), frequency = 4)
```

There are no missing values. The data is quarterly from 1999Q4 to 2016Q4 with no missing quarters inbetween. The data lie between 0.7 and 9.5%.

```{r}
autoplot(decompose(series1)) +
  labs(title="Decomposed E-Commerce Retail Sales as a Percent of Total Sales") +
  scale_y_continuous(labels = scales::percent)

series1 %>% ndiffs()
```

There is a clear upward trend in the data with fairly strong seasonal variation. It also has an increasing variance over time. The variance could be stablized by a transform (e.g. log seemed to do a good job stablizing the variance here), but could also be stablized by differencing as well. `ndiffs` indicates a first difference would be a good start according to the KPSS test.

```{r}
p1 = autoplot(acf(series1, plot = FALSE)) +
  ggtitle("Correlation Functions")

p2 = autoplot(pacf(series1, plot = FALSE)) +
  ggtitle("")

grid.arrange(p1, p2, ncol=2)
```

The ACF also points to a strong trend given it remains highly significant for many lags. The PACF also indicates seasonality.

Let's first attempt to remove the trend with differencing.

```{r}
series_diff1 <- diff(series1)
autoplot(decompose(series_diff1)) +
  labs(title="Decomposed, First Differenced\nE-Commerce Retail Sales as a Percent of Total Sales") +
  scale_y_continuous(labels = scales::percent)
series_diff1 %>% nsdiffs()
```

A lot of the trend is removed by the first difference, but there is still a strong seasonal component we can likely remove with a first order seasonal differencing. `nsdiffs` also indicates a single seasonal difference could help achieve stationarity:

```{r}
series_diff1_diff4 <- diff(series_diff1,4)
autoplot(decompose(series_diff1_diff4)) +
  labs(title="Decomposed, First Differenced, First-Order Seasonal Differenced\nE-Commerce Retail Sales as a Percent of Total Sales") +
  scale_y_continuous(labels = scales::percent)

series_diff1_diff4 %>% ur.kpss()
```

After these differences, the time series looks roughly stationary with not strong trend and roughly stable variance (so no need to transform the series). The KPSS test also indicates that there is not sufficient evidence to reject the null hypothesis that the data are stationary.

```{r}
plot_cfs <- function(ts,title){
  p1 = autoplot(acf(ts, plot = FALSE)) +
  ggtitle(title)

p2 = autoplot(pacf(ts, plot = FALSE)) +
  ggtitle("")

grid.arrange(p1, p2, ncol=2)
}

plot_cfs(series_diff1_diff4,"Correlation Functions\nDifferenced Series")

```
The pacf and acf of the differenced series are both significant at the first lag and another lag roughly 2 years back. This is inidicative of a low order $p$ and maybe $q$ to take care of the first lag and low order $P$ and $Q$ (perhaps 2 because the lags are about 2 years back) to take care of the later lag in our SARIMA model. 

## Model Construction

Our exploratory data analysis points to a SARIMA model with $d=1$ and $D=1$. It also points to maximum values of the other parameters as $p=1$, $q=1$, $P=2$, $Q=2$. Let's fit that model, a few around it with fewer parameters, and one with `auto.arima` to compare.

```{r}
# question requests fits using pre-2015 data
series1_train <- series1 %>% window(end=c(2014,4))
series1_test <- series1 %>% window(start=2015)

# train models
  # all models up to the max according to EDA
models <- list()
i <- 1
model_pdqPDQ <- c()
num_params <- c()
for (p in 0:1){
  for (q in 0:1){
    for (P in 0:2){
      for (Q in 0:2){
        models[[i]] <- series1_train %>% Arima(order=c(p,1,q),
                                               seasonal = c(P,1,Q),
                                               method='ML')
        model_pdqPDQ <- c(model_pdqPDQ,
                          glue("({p},1,{q})({P},1,{Q})"))
        num_params <- c(num_params,p+1+q+P+1+Q)
        i <- i+1
      }
    }
  }
}
  # add the auto arima model
auto.arima.model <- series1_train %>% auto.arima(stepwise=FALSE, approximation=FALSE)
models[[i]] <- auto.arima.model
model_pdqPDQ <- c(model_pdqPDQ, "auto.arima (0,1,1),(1,1,2)")
num_params <- c(num_params,5)


# model evaluation
# function to evaluate models
evaluate_model <- function(sarima_model, model_pdqPDQ, num_params){
  m_forecast <- sarima_model %>% forecast(8)
  rmse <- sqrt(mean((m_forecast$mean - series1_test)^2))
  return(c(model_pdqPDQ,num_params,rmse,
           sarima_model$aic,
           sarima_model$aicc,
           sarima_model$bic
           ))
}
model_info <- tibble(
  sarima_model=models,
  model_pdqPDQ=model_pdqPDQ,
  num_params=num_params
)

model_info <- 
  model_info %>%
  pmap(evaluate_model) %>%
  bind_cols() %>% 
  t() %>% as.tibble()
names(model_info) <- c("model","num_params","rmse for 2015-2016 prediction","aic","aicc","bic")

model_info %>% arrange(num_params,model)
```

Auto arima finds the model with the best aic. The models with fewer parameters seem to do better on the out of sample forecast though. Let's take a look at the residuals.

```{r}
plot_cfs(series1_train %>% 
           Arima(order=c(0,1,0), seasonal = c(2,1,0), method='ML') 
         %>% residuals(),
         "(0,1,0),(2,1,0) Residuals\nstill autocorrelated")

plot_cfs(auto.arima.model %>% residuals(),
         "Auto Arima (0,1,1),(1,1,2) Residuals\not autocorrelated")
```

Given the remaining autocorrelation on simpler models (only one shown above, both others checked appeared similar), we believe the auto.arima SARIMA(0,1,0),(1,1,1) model is best.

```{r}
checkresiduals(auto.arima.model)
```

The auto arima residuals look like white noise.

## Forecast

With our model, we now forecast through 2017.

```{r}
df %>% head()

auto.arima.model %>% forecast(h=12) %>% autoplot() +
  scale_y_continuous(labels = scales::percent)
```

The predictions seem to follow the trend very well.


# Question 2: Learning how to use the xts library

## Materials covered in Question 2 of this lab

  - Primarily the references listed in this document:

      - "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich. 2008. (xts.pdf)
      - "xts FAQ" by xts Development Team. 2013 (xts_faq.pdf)
      - xts_cheatsheet.pdf

# Task 1:

  1. Read 
    A. The **Introduction** section (Section 1), which only has 1 page of reading of xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    B. The first three questions in"xts FAQ"
        a. What is xts?
        b. Why should I use xts rather than zoo or another time-series package?
        c. HowdoIinstallxts?
    C. The "A quick introduction to xts and zoo objects" section in this document
        
  2. Read the "A quick introduction to xts and zoo objects" of this document

# A quick introduction to xts and zoo objects

### xts
```xts```
  - stands for eXtensible Time Series
  - is an extended zoo object
  - is essentially matrix + (time-based) index (aka, observation + time)

  - xts is a constructor or a subclass that inherits behavior from parent (zoo); in fact, it extends the popular zoo class. As such. most zoo methods work for xts
  - is a matrix objects; subsets always preserve the matrix form
  - importantly, xts are indexed by a formal time object. Therefore, the data is time-stamped
  - The two most important arguments are ```x``` for the data and ```order.by``` for the index. ```x``` must be a vector or matrix. ```order.by``` is a vector of the same length or number of rows of ```x```; it must be a proper time or date object and be in an increasing order

# Task 2:

  1. Read 
    A. Section 3.1 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
    B. The following questions in "xts FAQ"
        a. How do I create an xts index with millisecond precision?
        b. OK, so now I have my millisecond series but I still canâ€™t see the milliseconds displayed. What went wrong?

  2. Follow the following section of this document


# Creating an xts object and converting to an xts object from an imported dataset

We will create an `xts` object from a matrix and a time index. First, let's create a matrix and a time index.  The matrix, as it creates, is not associated with the time indext yet.

```{r setup, include=FALSE}
rm(list = ls())
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


```{r}
# Create a matrix
x <- matrix(rnorm(200), ncol=2, nrow=100)
colnames(x) <- c("Series01", "Series02")
str(x)
head(x,10)

idx <- seq(as.Date("2015/1/1"), by = "day", length.out = 100)
str(idx)
head(idx)
tail(idx)
```

In a nutshell, `xts` is a matrix indexed by a time object. To create an xts object, we "bind" the object with the index.  Since we have already created a matrix and a time index (of the same length as the number of rows of the matrix), we are ready to "bind" them together. We will name it *X*.

```{r}
library(xts)
X <- xts(x, order.by=idx)
str(X)
head(X,10)
```
As you can see from the structure of an `xts` objevct, it contains both a data component and an index, indexed by an objevct of class `Date`.

**xtx constructor**
```
xts(x=Null,
    order.by=index(x),
    frequency=NULL,
    unique=NULL,
    tzone=Sys.getenv("TZ"))
```
As mentioned previous, the two most important arguments are ```x``` and ```order.by```.  In fact, we only use these two arguments to create a xts object before.


With a xts object, one can decompose it.

### Deconstructing xts
```coredata()``` is used to extract the data component
```{r}
head(coredata(X),5)
```

```index()``` is used to extract the index (aka times)
```{r}
head(index(X),5)
```
  
### Conversion to xts from other time-series objects

We will use the same dataset "bls_unemployment.csv" that we used in the last live session to illustarte the functions below.


```{r}
df <- read.csv("bls_unemployment.csv", header=TRUE, stringsAsFactors = FALSE)

# Examine the data structure
  str(df)
  names(df)
  head(df)
  tail(df)

#table(df$Series.id, useNA = "always")
#table(df$Period, useNA = "always")

# Convert a column of the data frame into a time-series object
unemp <- ts(df$Value, start = c(2007,1), end = c(2017,1), frequency = 12)
  str(unemp)
  head(cbind(time(unemp), unemp),5)

# Now, let's convert it to an xts object
df_matrix <- as.matrix(df)
  head(df_matrix)
  str(df_matrix)
  rownames(df)

unemp_idx <- seq(as.Date("2007/1/1"), by = "month", length.out = 
length(df[,1]))
  head(unemp_idx)

unemp_xts <- xts(df$Value, order.by = unemp_idx)
  str(unemp_xts)
  head(unemp_xts)
```

# Task 3:

  1. Read 
    A. Section 3.2 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
  2. Follow the following section of this document
  
# Merging and modifying time series

One of the key strengths of ```xts``` is that it is easy to join data by column and row using a only few different functions. It makes creating time series datasets almost effortless.

The important criterion is that the xts objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.

The major functions is ```merge```.  It works like ```cbind``` or SQL's ```join```:

Let's look at an example. It assumes that you are familiar with concepts of inner join, outer join, left join, and right join.

```{r}
library(quantmod)
getSymbols("TWTR")
head(TWTR)
str(TWTR)
```

Note that the date obtained from the getSymbols function of the quantmod library is already an xts object.  As such, we can merge it directly with our unemployment rate xts object constructed above. Nevertheless, it is instructive to examine the data using the View() function to ensure that you understand the number of observations resulting from the joined series.

```{r}
# 1. Inner join
TWTR_unemp01 <- merge(unemp_xts, TWTR, join = "inner")
  str(TWTR_unemp01)
  head(TWTR_unemp01)

# 2. Outer join (filling the missing observations with 99999)
# Basic argument use
TWTR_unemp02 <- merge(unemp_xts, TWTR, join = "outer", fill = 99999)
  str(TWTR_unemp02)
  head(TWTR_unemp02)
  #View(TWTR_unemp02)

# Left join
TWTR_unemp03 <- merge(unemp_xts, TWTR, join = "left", fill = 99999)
  str(TWTR_unemp03)
  head(TWTR_unemp03)
  #View(TWTR_unemp03)
  
# Right join
TWTR_unemp04 <- merge(unemp_xts, TWTR, join = "right", fill = 99999)
  str(TWTR_unemp04)
  head(TWTR_unemp04)
  #View(TWTR_unemp04)
```

# Missing value imputation
xts also offers methods that allows filling missing values using last or previous observation. Note that I include this simply to point out that this is possible. I by no mean certify that this is the preferred method of imputing missing values in a time series.  As I mentioned in live session, the specific method to use in missing value imputation is completely context dependent.

Filling missing values from the last observation
```{r}
# First, let's replace the "99999" values with NA and then exammine the series. 

# Let's examine the first few dozen observations with NA
TWTR_unemp02['2013-10-01/2013-12-15'][,1]

# Replace observations with "99999" with NA and store in a new series
unemp01 <- TWTR_unemp02[, 1]
unemp01['2013-10-01/2013-12-15']
str(unemp01)
head(unemp01)
#TWTR_unemp02[, 1][TWTR_unemp02[, 1] >= 99990] <- NA

unemp02 <- unemp01
unemp02[unemp02 >= 99990] <- NA

cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'])

# Impute the missing values (stored as NA) with the last observation
TWTR_unemp02_v2a <- na.locf(TWTR_unemp02[,1], 
                            na.rm = TRUE, fromLast = TRUE) 
unemp03 <- unemp02
unemp03 <- na.locf(unemp03, na.rm = TRUE, fromLast = FALSE);

# Examine the pre- and post-imputed series
cbind(TWTR_unemp02['2013-10-01/2013-12-30'][,1], TWTR_unemp02_v2a['2013-10-01/2013-12-15'])

cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'],
unemp03['2013-10-01/2013-12-15'])
```

Another missing value imputation method is linear interpolation, which can also be easily done in xts objects. In the following example, we use linear interpolation to fill in the NA in between months.  The result is stored in ```unemp04```. Note in the following the different ways of imputing missing values.

```{r}
unemp04 <- unemp02
unemp04['2013-10-01/2014-02-01']
unemp04 <- na.approx(unemp04, maxgap=31)
unemp04['2013-10-01/2014-02-01']

round(cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'],
unemp03['2013-10-01/2013-12-15'],
unemp04['2013-10-01/2013-12-15']),2)
```

## Calculate difference in time series
A very common operation on time series is to take a difference of the series to transform a non-stationary serier to a stationary series. First order differencing takes the form $x(t) - x(t-k)$ where $k$ denotes the number of time lags. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).

Let's use the ```unemp_xts``` series as examples:
```{r}
# str(unemp_xts)
# unemp_xts
 
# diff(unemp_xts, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)

# calculate the first difference of AirPass using lag and subtraction
#AirPass - lag(AirPass, k = 1)

# calculate the first order 12-month difference if AirPass
# diff(unemp_xts, lag = 12, differences = 1)

cbind(unemp_xts, 
      diff(unemp_xts, lag = 1, difference = 1, log = FALSE, na.pad = TRUE),
      diff(unemp_xts, lag = 12, differences = 1)) %>% head(20)
```

# Task 4:

  1. Read 
    A. Section 3.4 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
    B. the following questions in "xts FAQ"
        a. I am using apply() to run a custom function on my xts series. Why the returned matrix has different dimensions than the original one?

  2. Follow the following two sections of this document

# Apply various functions to time series

The family of ```apply``` functions perhaps is one of the most powerful R function families. In time series, ```xts``` provides ```period.apply```, which takes (1) a time series, (2) an index of endpoints, and (3) a function to apply.  It takes the following general form:
```
period.apply(x, INDEX, FUN, ...)
```

As an example, we use the Twitter stock price series (to be precise, the daily closing price), create an index storing the points corresopnding to the weeks of the daily series, and apply functions to calculate the weekly mean. 

```{r}
# Step 1: Identify the endpoints; in this case, we use weekly time interval. That is, we extract the end index on each week of the series

#View(TWTR)
head(TWTR)
TWTR_ep <- endpoints(TWTR[,4], on = "weeks")
TWTR_ep

# Step 2: Calculate the weekly mean
TWTR.Close_weeklyMean <- period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = mean)
head(round(TWTR.Close_weeklyMean,2),8)
```

The power of the apply function really comes with the use of custom-defined function. For instance, we can easily 

```{r}
f <- function(x) {
  mean <- mean(x)
  quantile <- quantile(x,c(0.05,0.25,0.50,0.75,0.95))
  sd <- sd(x)
  
  result <- c(mean, sd, quantile)
  return(result)
}
head(round(period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = f),2),10)
```

# Calculate basic rolling statistics  of series by month
Using ```rollapply```, one can calculate rolling statistics of a series:

```{r}
# Calculate rolling mean over a 10-day period and print it with the original series
head(cbind(TWTR[,4], rollapply(TWTR[, 4], 10, FUN = mean, na.rm = TRUE)),15)
```

# Task 5:
## 1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames
```{r}
AMAZ_df <- read_csv(file = 'AMAZ.csv')
UMCSENT_df <- read_csv(file = 'UMCSENT.csv')
AMAZ_df %>% head()
UMCSENT_df %>% head()
```

## 2. Convert them to xts objects

```{r}
AMAZ <- xts(x = AMAZ_df %>% select(-Index), order.by = AMAZ_df$Index)
UMCSENT <- xts(x = UMCSENT_df$UMCSENT, order.by = UMCSENT_df$Index)
```


## 3. Merge the two set of series together, perserving all of the obserbvations in both set of series

### a. fill all of the missing values of the UMCSENT series with -9999

```{r}
stocks <- merge.xts(AMAZ,UMCSENT,join = "outer", fill = -9999)
```

###  b. then create a new series, named UMCSENT02, from the original  UMCSENT series replace all of the -9999 with NAs

```{r}
UMCSENT02 <- ifelse(stocks[,"UMCSENT"]==-9999, NA, stocks[,"UMCSENT"])
```

    
### c. then create a new series, named UMCSENT03, and replace the NAs with the last observation

My interpretation of "last" is the observation the latest in time that occurs before the target row that does not have an NA value.

```{r}
UMCSENT03 <- na.locf(UMCSENT02) 
```

### d. then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.

```{r}
UMCSENT04 <- na.approx(UMCSENT02)
```

### e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)

```{r}
stocks <- merge.xts(AMAZ,UMCSENT,join = "outer", fill = -9999)
stocks <- merge.xts(stocks, UMCSENT02, UMCSENT03, UMCSENT04, fill = -9999)
stocks['2009-01-01/2009-02-01']
plot(stocks['2009-01-01/2009-02-01'][,"UMCSENT.3"],main="Linear Interpolation Check")
```
Printing the observations from january 2009 to the first day of Feb we see the values were fed forward. The plot shows the linear interpolation also occured correctly.

## 4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

In order to get the daily return we need data at the daily level:

```{r}
# make another series with daily frequency that has a meaningless value
#   purpose is to get stocks index to be daily
date_range <- seq(as.Date(min(index(AMAZ))), as.Date(max(index(AMAZ))), by = "day")
daily <- xts(rep(1,length(date_range)),order.by = date_range)
stocks <- merge.xts(stocks,daily, fill = -9999)
stocks %>% head()
```

```{r}
plot(AMAZ[,'AMAZ.Close'])
```


Next we need to handle missing values for the Amazon close values. From the plot above it seems like the time series frequency is high enough that a linear interpolation would be a good approximation of the NA values:

```{r}
# pull close data for all days replacing -9999 with NA
AMAZ.Close <- ifelse(stocks[,"AMAZ.Close"]==-9999, NA, stocks[,"AMAZ.Close"])
# cut to range amazon had data for
AMAZ.Close <- AMAZ.Close[paste0(min(index(AMAZ)),'/',max(index(AMAZ)))]
# interpolate missing values
AMAZ_fixed <- na.approx(AMAZ.Close)
```

Now we can calculate and plot the daily return.

```{r warning=F}
AMAZ_daily_return <- (AMAZ_fixed - lag(AMAZ_fixed))/lag(AMAZ_fixed)

AMAZ_daily_return %>%
  ggplot(aes(y=AMAZ.Close,x=index(AMAZ_daily_return))) +
  geom_line() + scale_y_continuous(labels = scales::percent) +
  labs(title="Amazon Daily Return",
       subtitle=glue("from {min(index(AMAZ))} to {max(index(AMAZ))}"),
       x="date",y="daily return")

```



## 5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.

We choose to average over X-day intervals and just remove missing observations. That way we can interpret every point as the average of a range the same width even though each average may have a different number of points within it.

```{r}
# 20 day rolling average
AMAZ_20_day_rolling_mean <- rollapply(AMAZ.Close, 20, FUN = mean, na.rm = TRUE)
names(AMAZ_20_day_rolling_mean) <- "AMAZ.Close.20.Day.Rolling.Average"
# 50 day
AMAZ_50_day_rolling_mean <- rollapply(AMAZ.Close, 50, FUN = mean, na.rm = TRUE)
names(AMAZ_50_day_rolling_mean) <- "AMAZ.Close.50.Day.Rolling.Average"
# plot together
AMAZ_close_w_means <- merge.xts(AMAZ.Close,AMAZ_20_day_rolling_mean,AMAZ_50_day_rolling_mean)
plot(AMAZ_close_w_means,main="AMAZ Close Price with Rolling Averages", legend.loc='topright')
legend("topright",
       c("Actual Close Price", "20 Day Rolling Average", "50 Day Rolling Average"),
       col = c(1,2,3)
       )
```


# Dan Q1 old did log transform but didnt work out as well as just differencing


First, let's see if we can stablize the variance by applying a log10 transform (we use base 10 to maintain interpretability though other bases performed similarly).
```{r}
df %>% 
  ggplot(aes(x=DATE,y=ECOMPCTNSA/100)) +
  geom_line()+
  geom_smooth() +
  labs(title="Log Stablizes Variance",
       y="E-Commerce Retail Sales as a Percent of Total Sales") +
scale_y_log10(labels = scales::percent)
```

The log transform does a good job of stablizing the variance. Now, let's see if a first difference will remove the trend.

```{r}
logseries1 <- ts(log10(df$ECOMPCTNSA/100), start=c(1999,10), frequency = 4)
autoplot(decompose(diff(logseries1))) +
  labs(title="Decomposed, First-Differenced, Log\nE-Commerce Retail Sales as a Percent of Total Sales")

```

A small trend remains, but much less than before and the differenced series largly matches the seasonal component series. This indicates we will likely need a seasonal difference as well. 

```{r}
# keep first diffed series
logseries1_diff1 <- diff(logseries1)

# add a seasonal difference
autoplot(decompose(diff(logseries1_diff1,4)))
autoplot(decompose(diff(diff(series1),4)))
```
